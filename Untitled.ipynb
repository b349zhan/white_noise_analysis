{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8c4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.model1 import Model1\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "%run -i \"./utils/data_model_loader.py\"\n",
    "%run -i \"./utils/conf_loader.py\"\n",
    "conf = load_config(\"conf.txt\")\n",
    "cuda = torch.cuda.is_available()\n",
    "conf[\"cuda\"] = cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "538cf6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"MNIST\"\n",
    "train_dataset, test_dataset, train_loader, test_loader = get_loader(dataset_name, transforms.ToTensor(), conf, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "050436fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name, train_loader, test_loader,conf, dataset_name = \"MNIST\"):\n",
    "    '''\n",
    "    '''    \n",
    "    if model_name == \"model1\":\n",
    "        model = Model1(image_size = 28, image_channel = 1, class_num = 10)\n",
    "    else:\n",
    "        model = Model2(image_size = 28, image_channel = 1, class_num = 10)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3) \n",
    "    losses = []\n",
    "    best_accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(conf[\"TRAIN_EPOCHS\"]):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(data) \n",
    "            loss = F.cross_entropy(y_pred, target)\n",
    "            losses.append(loss.cpu().data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 1:\n",
    "                print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch+1,\n",
    "                    conf[\"TRAIN_EPOCHS\"],\n",
    "                    batch_idx * len(data), \n",
    "                    len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), \n",
    "                    loss.cpu().data), \n",
    "                    end='')\n",
    "        evaluate_x = Variable(test_loader.dataset.data.type_as(torch.FloatTensor()))\n",
    "        evaluate_y = Variable(test_loader.dataset.targets)\n",
    "        if cuda: evaluate_x, evaluate_y = evaluate_x.cuda(), evaluate_y.cuda()\n",
    "\n",
    "        model.eval()\n",
    "        output = model(evaluate_x[:,None,...])\n",
    "        pred = output.data.max(1)[1]\n",
    "        d = pred.eq(evaluate_y.data).cpu()\n",
    "        accuracy = d.sum().type(dtype=torch.float64)/d.size()[0]\n",
    "\n",
    "        print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Test Accuracy: {:.4f}%'.format(\n",
    "            epoch+1,\n",
    "            conf[\"TRAIN_EPOCHS\"],\n",
    "            len(train_loader.dataset), \n",
    "            len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), \n",
    "            loss.cpu().data,\n",
    "            accuracy*100,\n",
    "            end=''))\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save({'epoch': epoch,\n",
    "                            'model': model.state_dict(),\n",
    "                            'optimizer': optimizer.state_dict()\n",
    "                           }, f'{conf[\"SAVE_PATH\"]}/{dataset_name}_model1_best.pth')\n",
    "            print('\\r Best model saved.\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb73e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 1/5 [19264/60000 (32%)]\tLoss: 0.256724"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model_name, train_loader, test_loader, conf, dataset_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(y_pred, target)\n\u001b[1;32m     22\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/python-env/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/python-env/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\"model1\", train_loader, test_loader, conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python-env] *",
   "language": "python",
   "name": "conda-env-python-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
